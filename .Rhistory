tk_get_timeseries_signature()
new_data_tbl
# Make predictions
pred <- predict(fit_lm, newdata = select(new_data_tbl, -c(index, diff)))
predictions_tbl <- tibble(
Index  = future_idx,
value = pred
)
predictions_tbl
split <- round(nrow(SUM_DATA) * .90)
datat_to <- SUM_DATA[1:split,]
actuals_tbl <- SUM_DATA[(split + 1):nrow(SUM_DATA),]
#colnames(actuals_tbl)[2] <- "value"
p<-ggplot(SUM_DATA,aes(x=Index,y=SUM))+
# Training data
geom_line(color = palette_light()[[1]]) +
geom_point(color = palette_light()[[1]])+
# Predictions
geom_line(aes(y = value), color = palette_light()[[4]], data = predictions_tbl) +
geom_point(aes(y = value), color = palette_light()[[4]], data = predictions_tbl)+
# Actuals
geom_line(aes(y = SUM),color = palette_light()[[3]], data = actuals_tbl) +
geom_point(aes(y = SUM),color = palette_light()[[3]], data = actuals_tbl)+
theme_tq() +
labs(title = "Time series sum data")
ggplotly(p)
error_tbl <- left_join(actuals_tbl, predictions_tbl) %>%
rename(actual = SUM, pred = value) %>%
mutate(
error     = actual - pred,
error_pct = error / actual
)
error_tbl
# Calculating test error metrics
test_residuals <- error_tbl$error
test_error_pct <- error_tbl$error_pct * 100 # Percentage error
me   <- mean(test_residuals, na.rm=TRUE)
rmse <- mean(test_residuals^2, na.rm=TRUE)^0.5
mae  <- mean(abs(test_residuals), na.rm=TRUE)
mape <- mean(abs(test_error_pct), na.rm=TRUE)
mpe  <- mean(test_error_pct, na.rm=TRUE)
tibble(me, rmse, mae, mape, mpe) %>% glimpse()
# Coerce to xts
beer_sales_xts <- tk_xts(SUM_DATA)
# Show the first six rows of the xts object
beer_sales_xts %>%
head()
tk_tbl(beer_sales_xts, rename_index = "date")
# Coerce to ts
beer_sales_ts <- tk_ts(SUM_DATA)
# Show the calendar-printout
beer_sales_ts
tk_tbl(beer_sales_ts, rename_index = "date")
has_timetk_idx(beer_sales_ts)
# If timetk_idx is present, can get original dates back
tk_tbl(beer_sales_ts, timetk_idx = TRUE, rename_index = "date")
runApp()
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Randomly order data: Sonar
data1 <- data1[rows,]
# Identify row to split on: split
split <- round(nrow(data1) * .60)
# Create train
train <- data1[1:split,]
# Create test
test <- data1[(split + 1):nrow(data1),]
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- data1[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- data1[validation_index,]
# a) linear algorithms
set.seed(7)
fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(SP10 ~ Date_Time,data=data1, method="rpart", metric=metric, trControl=control)
set.seed(7)
fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
set.seed(7)
fit.cart <- train(SP10 ~ Date_Time,data=data1, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(SP10 ~ Date_Time,data=data1, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.svm <- train(SP10 ~ Date_Time,data=data1, method="svmRadial", metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
print(fit.lda)
# Shuffle row indices: rows
rows <- sample(nrow(data1)
# Randomly order data: Sonar
data1 <- data1[rows,]
# Shuffle row indices: rows
rows <- sample(nrow(data1)
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Randomly order data: Sonar
data1 <- data1[rows,]
# Identify row to split on: split
split <- round(nrow(data1) * .60)
train <- data1[1:split,]
test <- data1[(split + 1):nrow(data1),]
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- data1[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- data1[validation_index,]
# a) linear algorithms
set.seed(7)
fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Randomly order data: Sonar
data1 <- data1[rows,]
# Identify row to split on: split
split <- round(nrow(data1) * .60)
# Create train
train <- data1[1:split,]
# Create test
test <- data1[(split + 1):nrow(data1),]
#------------------------------------------
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric = ifelse(is.factor(y), "Accuracy", "RMSE")
#metric <- "Accuracy"
validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- data1[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- data1[validation_index,]
# a) linear algorithms
set.seed(7)
fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
fit.ldp<-lda(SP10 ~ Date_Time,data=data1)
fit.cart <- train(SP10 ~ Date_Time,data=data1, method="rpart", metric=metric, trControl=control)
set.seed(7)
fit.knn <- train(SP10 ~ Date_Time,data=data1, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.svm <- train(SP10 ~ Date_Time,data=data1, method="svmRadial", metric=metric, trControl=control)
set.seed(7)
fit.rf <- train(SP10 ~ Date_Time,data=data1, method="rf", metric=metric, trControl=control)
use warnings()
warnings()
set.seed(7)
fit.svm <- train(SP10 ~ Date_Time,data=data1, method="svmRadial", metric=metric, trControl=control)
set.seed(7)
fit.rf <- train(SP10 ~ Date_Time,data=data1, method="rf", metric=metric, trControl=control)
#summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
print(fit.lda)
fit.ldp<-lda(SP10 ~ Date_Time,data=data1)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
fit.lda<-lda(SP10 ~ Date_Time,data=data1)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
fit.lda<-lda(SP10 ~ Date_Time,data=data1,metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
print(fit.lda)
summary(results)
dotplot(results)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
fit.lda<-lda(SP10 ~ Date_Time,data=data1,metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Randomly order data: Sonar
data1 <- data1[rows,]
# Identify row to split on: split
split <- round(nrow(data1) * .60)
# Create train
train <- data1[1:split,]
# Create test
test <- data1[(split + 1):nrow(data1),]
#------------------------------------------
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric = ifelse(is.factor(y), "Accuracy", "RMSE")
#metric <- "Accuracy"
validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- data1[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- data1[validation_index,]
# a) linear algorithms
set.seed(7)
#fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
#fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
fit.lda<-lda(SP10 ~ Date_Time,data=data1,metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(SP10 ~ Date_Time,data=data1, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(SP10 ~ Date_Time,data=data1, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(SP10 ~ Date_Time,data=data1, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(SP10 ~ Date_Time,data=data1, method="rf", metric=metric, trControl=control)
#summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
print(fit.lda)
dotplot(results)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$CHANNEL)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$CHANNEL)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$CHANNEL)
validation
validation$CHANNEL
folder <- "."
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder
# read in each .csv file in file_list and rbind them into a data frame called data1
data1 <-
do.call("rbind",
lapply(file_list,
function(x)
read_csv(paste(folder, x, sep=''))))
#====================Preprocesing===============================
#chnge format of Date
data1$Date_Time <- data1$Date
data1$Date_Time <- do.call(paste,c(data1[c("Date_Time","Time")],sep = ""))
data1$Date_Time <- as.POSIXct(data1$Date_Time,format = "%d%m%y%H%M")
#data1$Date_Time <- as.Date(data1$Date_Time, "%d%m%y%H%M")
#data1$Date <- as.POSIXct(data1$Date,format = "%d%m%y")
data1<-data1[,c(ncol(data1),1:(ncol(data1)-1))]
#Add ":" in time
data1$Time <- sub("(.{2})(.*)", "\\1:\\2", data1$Time)
#Necessarily Columns as.factor
data1$CHANNEL <- as.numeric(data1$CHANNEL)
data1$LOCATION <- as.factor(data1$LOCATION)
data1$SITE <- as.factor(data1$SITE)
data1$FILENAME <- as.factor(data1$FILENAME)
data1$INSTRUMENT <- as.factor(data1$INSTRUMENT)
data1$HEADINGS <- as.factor(data1$HEADINGS)
#data1$Time <- as.numeric(data1$Time)
#set measured data as numeric
data1$SP1 = as.numeric(data1$SP1)
data1$SP2 = as.numeric(data1$SP2)
data1$SP3 = as.numeric(data1$SP3)
data1$SP4 = as.numeric(data1$SP4)
data1$SP5 = as.numeric(data1$SP5)
data1$SP6 = as.numeric(data1$SP6)
data1$SP7 = as.numeric(data1$SP7)
data1$SP8 = as.numeric(data1$SP8)
data1$SP9 = as.numeric(data1$SP9)
data1$SP10 = as.numeric(data1$SP10)
data1$SP11 = as.numeric(data1$SP11)
data1$SP12 = as.numeric(data1$SP12)
data1$SP13 = as.numeric(data1$SP13)
data1$SP14 = as.numeric(data1$SP14)
data1$LN1 = as.numeric(data1$LN1)
data1$LN2 = as.numeric(data1$LN2)
data1$LN3 = as.numeric(data1$LN3)
data1$LN4 = as.numeric(data1$LN4)
data1$LN5 = as.numeric(data1$LN5)
data1$LN6 = as.numeric(data1$LN6)
data1$LN7 = as.numeric(data1$LN7)
data1$LN8 = as.numeric(data1$LN8)
data1$LN9 = as.numeric(data1$LN9)
data1$CS1 = as.numeric(data1$CS1)
data1$CS2 = as.numeric(data1$CS2)
data1$CS3 = as.numeric(data1$CS3)
data1$CS4 = as.numeric(data1$CS4)
data1$CS5 = as.numeric(data1$CS5)
data1$CS6 = as.numeric(data1$CS6)
#add column sum of cars in interval
data1$SUM<-rowSums(data1[,5:18])
#unfinished preparation I will explain what I need
splitH<-str_split_fixed(data1$HEADINGS, " ", 4)
splitH <- as.data.frame(splitH)
splitH <- unique(splitH)
data1$CHANNEL1 <- data1$CHANNEL
colnames(splitH) <- "CHANNEL1"
colnames(splitH)[2] <- "CHANNEL2"
colnames(splitH)[3] <- "CHANNEL3"
colnames(splitH)[4] <- "CHANNEL4"
splitH$rowNames <-row.names.data.frame(splitH)
# replace.if
#
# for (i in vector) {
# data1$CHANNEL<-gsub("1", splitH[1,1], data1$CHANNEL)
# data1$CHANNEL<-gsub("2", splitH[1,2], data1$CHANNEL)
# data1$CHANNEL<-gsub("3", splitH[1,3], data1$CHANNEL)
# data1$CHANNEL<-gsub("4", splitH[1,4], data1$CHANNEL)
# }
#data1$CHANNEL <- do.call(paste,c(data1[c("CHANNEL1","CHANNEL")],sep = "_"))
data1$CHANNEL = as.factor(data1$CHANNEL)
data1$Time = as.factor(data1$Time)
data1$CHANNEL1 <- NULL
filter<-filter(data1, CHANNEL == "PP",FILENAME == "SC1_MEJA")
df_XCV<-data1
abc <- df_XCV$Date_Time
df_XCV<-select_if(df_XCV, is.numeric)
df_XCV$Date_Time<- abc
df_XCV<-df_XCV[,c(ncol(df_XCV),1:(ncol(df_XCV)-1))]
#Create time series object
df_XCV_xts<-xts(df_XCV[, -1], order.by=as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ")))
ep <- endpoints(df_XCV_xts, on = "hours")
pac<-period.apply(df_XCV_xts[,(names(df_XCV_xts)) ], INDEX = ep, FUN = mean)
#Time series to DF
pac <-fortify(pac)
pac <-  select(pac, -c(PEAKINT, INTERVAL))
pac<-tk_augment_timeseries_signature(pac)
df_XCV<-data1
abc <- df_XCV$Date_Time
df_XCV<-select_if(df_XCV, is.numeric)
df_XCV$Date_Time<- abc
df_XCV<-df_XCV[,c(ncol(df_XCV),1:(ncol(df_XCV)-1))]
#Create time series object
df_XCV_xts<-xts(df_XCV[, -1], order.by=as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ")))
as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ")
as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ"))
as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ"))
df_XCV$Date_Time
df_XCV$Date_Time
df_XCV<-data1
abc <- df_XCV$Date_Time
abc
df_XCV<-data1
data()
data1
folder <- "."
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder
# read in each .csv file in file_list and rbind them into a data frame called data1
data1 <-
do.call("rbind",
lapply(file_list,
function(x)
read_csv(paste(folder, x, sep=''))))
folder <- "./"
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder
# read in each .csv file in file_list and rbind them into a data frame called data1
data1 <-
do.call("rbind",
lapply(file_list,
function(x)
read_csv(paste(folder, x, sep=''))))
data1$Date_Time <- data1$Date
data1$Date_Time <- do.call(paste,c(data1[c("Date_Time","Time")],sep = ""))
data1$Date_Time <- as.POSIXct(data1$Date_Time,format = "%d%m%y%H%M")
data1<-data1[,c(ncol(data1),1:(ncol(data1)-1))]
data1
data1$Time <- sub("(.{2})(.*)", "\\1:\\2", data1$Time)
data1$CHANNEL <- as.numeric(data1$CHANNEL)
data1$LOCATION <- as.factor(data1$LOCATION)
data1$SITE <- as.factor(data1$SITE)
data1$FILENAME <- as.factor(data1$FILENAME)
data1$INSTRUMENT <- as.factor(data1$INSTRUMENT)
data1$HEADINGS <- as.factor(data1$HEADINGS)
#data1$Time <- as.numeric(data1$Time)
#set measured data as numeric
data1$SP1 = as.numeric(data1$SP1)
data1$SP2 = as.numeric(data1$SP2)
data1$SP3 = as.numeric(data1$SP3)
data1$SP4 = as.numeric(data1$SP4)
data1$SP5 = as.numeric(data1$SP5)
data1$SP6 = as.numeric(data1$SP6)
data1$SP7 = as.numeric(data1$SP7)
data1$SP8 = as.numeric(data1$SP8)
data1$SP9 = as.numeric(data1$SP9)
data1$SP10 = as.numeric(data1$SP10)
data1$SP11 = as.numeric(data1$SP11)
data1$SP12 = as.numeric(data1$SP12)
data1$SP13 = as.numeric(data1$SP13)
data1$SP14 = as.numeric(data1$SP14)
data1$LN1 = as.numeric(data1$LN1)
data1$LN2 = as.numeric(data1$LN2)
data1$LN3 = as.numeric(data1$LN3)
data1$LN4 = as.numeric(data1$LN4)
data1$LN5 = as.numeric(data1$LN5)
data1$LN6 = as.numeric(data1$LN6)
data1$LN7 = as.numeric(data1$LN7)
data1$LN8 = as.numeric(data1$LN8)
data1$LN9 = as.numeric(data1$LN9)
data1$CS1 = as.numeric(data1$CS1)
data1$CS2 = as.numeric(data1$CS2)
data1$CS3 = as.numeric(data1$CS3)
data1$CS4 = as.numeric(data1$CS4)
data1$CS5 = as.numeric(data1$CS5)
data1$CS6 = as.numeric(data1$CS6)
#add column sum of cars in interval
data1$SUM<-rowSums(data1[,5:18])
#unfinished preparation I will explain what I need
splitH<-str_split_fixed(data1$HEADINGS, " ", 4)
splitH <- as.data.frame(splitH)
splitH <- unique(splitH)
data1$CHANNEL1 <- data1$CHANNEL
colnames(splitH) <- "CHANNEL1"
colnames(splitH)[2] <- "CHANNEL2"
colnames(splitH)[3] <- "CHANNEL3"
colnames(splitH)[4] <- "CHANNEL4"
splitH$rowNames <-row.names.data.frame(splitH)
# replace.if
#
# for (i in vector) {
# data1$CHANNEL<-gsub("1", splitH[1,1], data1$CHANNEL)
# data1$CHANNEL<-gsub("2", splitH[1,2], data1$CHANNEL)
# data1$CHANNEL<-gsub("3", splitH[1,3], data1$CHANNEL)
# data1$CHANNEL<-gsub("4", splitH[1,4], data1$CHANNEL)
# }
#data1$CHANNEL <- do.call(paste,c(data1[c("CHANNEL1","CHANNEL")],sep = "_"))
data1$CHANNEL = as.factor(data1$CHANNEL)
data1$Time = as.factor(data1$Time)
data1$CHANNEL1 <- NULL
#===========================AGGREGATION==========================
filter<-filter(data1, CHANNEL == "PP",FILENAME == "SC1_MEJA")
df_XCV<-data1
abc <- df_XCV$Date_Time
abc
df_XCV<-select_if(df_XCV, is.numeric)
df_XCV$Date_Time<- abc
df_XCV<-df_XCV[,c(ncol(df_XCV),1:(ncol(df_XCV)-1))]
#Create time series object
df_XCV_xts<-xts(df_XCV[, -1], order.by=as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ")))
ep <- endpoints(df_XCV_xts, on = "hours")
pac<-period.apply(df_XCV_xts[,(names(df_XCV_xts)) ], INDEX = ep, FUN = mean)
#Time series to DF
pac <-fortify(pac)
pac <-  select(pac, -c(PEAKINT, INTERVAL))
pac<-tk_augment_timeseries_signature(pac)
#source("Aggregation_Date.R")
#source("Aggregation_Date_Time.R")
# Shuffle row indices: rows
rows <- sample(nrow(data1))
# Randomly order data: Sonar
data1 <- data1[rows,]
# Identify row to split on: split
split <- round(nrow(data1) * .60)
# Create train
train <- data1[1:split,]
# Create test
test <- data1[(split + 1):nrow(data1),]
#------------------------------------------
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric = ifelse(is.factor(y), "Accuracy", "RMSE")
#metric <- "Accuracy"
validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- data1[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- data1[validation_index,]
# a) linear algorithms
set.seed(7)
#fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
#fit.lda <- train(SP10 ~ Date_Time,data=data1, method="lda", metric=metric, trControl=control)
fit.lda<-lda(SP10 ~ Date_Time,data=data1,metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(SP10 ~ Date_Time,data=data1, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(SP10 ~ Date_Time,data=data1, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(SP10 ~ Date_Time,data=data1, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(SP10 ~ Date_Time,data=data1, method="rf", metric=metric, trControl=control)
#summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
print(fit.lda)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$CHANNEL)
validation$CHANNEL
traceback()
sort.list(validation$CHANNEL)
predictions
sort.list(predictions)
predictions<-c(predictions)
sort.list(predictions)
predictions
predictions$x
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions$x, validation$CHANNEL)
predictions
predictions$posterior
predictions$x
confusionMatrix(predictions, validation$SP10)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$SP10)
predictions
dotplot(results)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
