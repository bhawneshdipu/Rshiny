) +
# 80% CI
geom_ribbon(
aes(
ymin = lo.80,
ymax = hi.80,
fill = key
),
fill = "#596DD5",
color = NA,
size = 0,
alpha = 0.8
) +
# Prediction
geom_line() +
geom_point() +
# Actuals
geom_line(aes_string(x = "Index", y = paste0(my_track)),
color = palette_light()[[1]],
data = actuals_tbl) +
geom_point(aes_string(x = "Index", y = paste0(my_track)),
color = palette_light()[[1]],
data = actuals_tbl) +
# Aesthetics
labs(
title = "Beer Sales Forecast: ARIMA",
x = "",
y = "Thousands of Tons",
subtitle = "sw_sweep tidies the auto.arima() forecast output"
) +
scale_color_tq() +
scale_fill_tq() +
theme_tq()
})
ggplotly(p)
## ARIMA ##
#====================================ARIMA + sweep: MAPE = 4.3% (sweep demo)==================================
SUM_DATA <- pac[,c(1,31)]
p<-SUM_DATA %>%
ggplot(aes(Index, SUM)) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA, n = 12, size = 1) +
theme_tq() %>%
labs(title = "Beer Sales: 2007 through 2016")
ggplotly(p)
beer_sales_ts <- tk_ts(SUM_DATA)
beer_sales_ts
has_timetk_idx(beer_sales_ts)
fit_arima <- auto.arima(beer_sales_ts)
fit_arima
# sw_tidy - Get model coefficients
sw_tidy(fit_arima)
# sw_glance - Get model description and training set accuracy measures
sw_glance(fit_arima) %>%
glimpse()
# sw_augment - get model residuals
sw_augment(fit_arima, timetk_idx = TRUE)
p<-sw_augment(fit_arima, timetk_idx = TRUE) %>%
ggplot(aes(x = index, y = .resid)) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +
labs(title = "Residual diagnostic") +
theme_tq()
ggplotly(p)
# Forecast next 12 months
fcast_arima <- forecast(fit_arima, h = 100)
class(fcast_arima)
# Check if object has timetk index
has_timetk_idx(fcast_arima)
# sw_sweep - tidies forecast output
fcast_tbl <- sw_sweep(fcast_arima, timetk_idx = TRUE)
fcast_tbl
# Visualize the forecast with ggplot
p<-fcast_tbl %>%
ggplot(aes(x = index, y = SUM, color = key)) +
# 95% CI
geom_ribbon(aes(ymin = lo.95, ymax = hi.95),
fill = "#D5DBFF", color = NA, size = 0) +
# 80% CI
geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key),
fill = "#596DD5", color = NA, size = 0, alpha = 0.8)+
# Prediction
geom_line() +
geom_point() +
# Actuals
geom_line(aes(x = Index, y = SUM), color = palette_light()[[1]], data = actuals_tbl) +
geom_point(aes(x = Index, y = SUM), color = palette_light()[[1]], data = actuals_tbl)+
# Aesthetics
labs(title = "Beer Sales Forecast: ARIMA", x = "", y = "Thousands of Tons",
subtitle = "sw_sweep tidies the auto.arima() forecast output") +
scale_color_tq() +
scale_fill_tq() +
theme_tq()
ggplotly(p)
error_tbl <- left_join(actuals_tbl, fcast_tbl, by = c("Index" = "index"))%>%
rename(actual = SUM.x, pred = SUM.y) %>%
select(Index, actual, pred) %>%
mutate(
error     = actual - pred,
error_pct = error / actual
)
error_tbl
na.omit(error_tbl)
# Calculate test error metrics
test_residuals <- error_tbl$error
test_error_pct <- error_tbl$error_pct * 100 # Percentage error
me   <- mean(test_residuals, na.rm=TRUE)
rmse <- mean(test_residuals^2, na.rm=TRUE)^0.5
mae  <- mean(abs(test_residuals), na.rm=TRUE)
mape <- mean(abs(test_error_pct), na.rm=TRUE)
mpe  <- mean(test_error_pct, na.rm=TRUE)
tibble(me, rmse, mae, mape, mpe) %>% glimpse()
shiny::runApp()
runApp()
runApp()
tryCatch({
p <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
labs(title = "Beer Sales: 2007 through 2016")
})
SUM_DATA <- pac[, c(1, grep(paste0(my_track), colnames(pac)))]
pac<-dipu.pac_data
SUM_DATA <- pac[, c(1, grep(paste0(my_track), colnames(pac)))]
#SUM_DATA <- pac[, c(1, 31)]
tryCatch({
p <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
labs(title = "Beer Sales: 2007 through 2016")
})
tryCatch({
beer_sales_ts <- tk_ts(SUM_DATA)
beer_sales_ts
has_timetk_idx(beer_sales_ts)
})
tryCatch({
beer_sales_ts <- tk_ts(SUM_DATA)
})
SUM_DATA <- pac[, c(1, grep(paste0(my_track), colnames(pac)))]
if (exists("my_track")==FALSE || is.null(my_track) || my_track == "") {
my_track = "SUM"
}
SUM_DATA <- pac[, c(1, grep(paste0(my_track), colnames(pac)))]
tryCatch({
p <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
labs(title = "Beer Sales: 2007 through 2016")
})
tryCatch({
beer_sales_ts <- tk_ts(SUM_DATA)
})
beer_sales_ts
has_timetk_idx(beer_sales_ts)
tryCatch({
fit_arima <- auto.arima(beer_sales_ts)
fit_arima
})
exists("fit_arima")
sw_tidy(fit_arima)
sw_glance(fit_arima) %>%
glimpse()
sw_augment(fit_arima, timetk_idx = TRUE)
tryCatch({
p <- sw_augment(fit_arima, timetk_idx = TRUE) %>%
ggplot(aes(x = index, y = .resid)) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +
labs(title = "Residual diagnostic") +
theme_tq()
})
ggplot(p)
ggplotly(p)
fcast_arima <- forecast(fit_arima, h = 100)
class(fcast_arima)
has_timetk_idx(fcast_arima)
fcast_tbl <- sw_sweep(fcast_arima, timetk_idx = TRUE)
fcast_tbl
split <- round(nrow(SUM_DATA) * .90)
datat_to <- SUM_DATA[1:split, ]
actuals_tbl <- SUM_DATA[(split + 1):nrow(SUM_DATA), ]
tryCatch({
# Visualize the forecast with ggplot
p <- fcast_tbl %>%
ggplot(aes_string(x = "index", y = paste0(my_track), color = paste0("key"))) +
# 95% CI
geom_ribbon(
aes(ymin = lo.95, ymax = hi.95),
fill = "#D5DBFF",
color = NA,
size = 0
) +
# 80% CI
geom_ribbon(
aes(
ymin = lo.80,
ymax = hi.80,
fill = key
),
fill = "#596DD5",
color = NA,
size = 0,
alpha = 0.8
) +
# Prediction
geom_line() +
geom_point() +
# Actuals
geom_line(aes_string(x = "Index", y = paste0(my_track)),
color = palette_light()[[1]],
data = actuals_tbl) +
geom_point(aes_string(x = "Index", y = paste0(my_track)),
color = palette_light()[[1]],
data = actuals_tbl) +
# Aesthetics
labs(
title = "Beer Sales Forecast: ARIMA",
x = "",
y = "Thousands of Tons",
subtitle = "sw_sweep tidies the auto.arima() forecast output"
) +
scale_color_tq() +
scale_fill_tq() +
theme_tq()
})
ggplotly(p)
devtools::install_github("tidyverse/ggplot2")
library(ggplot2)
runApp()
runApp()
tryCatch({
SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
labs(title = "Beer Sales: 2007 through 2016")
})
exists("p")
runApp()
runApp()
exists("p1")
runApp()
exists("p1")
p1 <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
labs(fill = "Beer Sales: 2007 through 2016")
exists("p1")
p1 <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
theme_tq() +
ggtitle("Beer Sales: 2007 through 2016")
exists("p1")
p1 <- SUM_DATA %>%
ggplot(aes_string("Index", paste0(my_track))) +
geom_line(col = palette_light()[1]) +
geom_point(col = palette_light()[1]) +
geom_ma(ma_fun = SMA,
n = 12,
size = 1) +
ggtitle("Beer Sales: 2007 through 2016")
exists("p1")
runApp()
runApp()
runApp()
error_tbl <-
left_join(actuals_tbl, fcast_tbl, by = c("Index" = "index")) %>%
rename_(actual = paste0(my_track,".","x"),pred=paste0(my_track,".","y")) %>%
select_("Index", "actual", "pred") %>%
mutate(error     = actual - pred,
error_pct = error / actual)
runApp()
getwd()
library(readr)
library(ggplot2)
library(plotly)
library(stringr)
library(DataLoader)
library(forecast)
library(tseries)
library(zoo)
library(xts)
library(fts)
library(MASS)
library(caret)
library(e1071)
library(dplyr)
#library(h2o)        # Awesome ML Library
library(timetk)     # Toolkit for working with time series in R
library(tidyquant)
library(anomalyDetection)
library(TSMining)
library(randomForest)
#install.packages("devtools")
#devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
packageVersion('plotly')
#==================================Motif discovery=======================================
# data(BuildOperation)
# res.wcc <- Func.motif(ts = BuildOperation$WCC, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = pac$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.ahu <- Func.motif(ts = pac$SP12, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)
#Visualization
data.wcc <- Func.visual.SingleMotif(single.ts = pac$SP11, window.size = 24, motif.indices = res.wcc$Indices)
data.ahu <- Func.visual.SingleMotif(single.ts = pac$SP12, window.size = 24, motif.indices = res.ahu$Indices)
#Determine the total number of motifs discovered in the time series of WCC
n <- length(unique(data.wcc$data.1$Y))
#Make the plot
p<-ggplot(data = data.wcc$data.1) +
geom_line(aes(x = 1:dim(data.wcc$data.1)[1], y = X)) +
geom_point(aes(x = 1:dim(data.wcc$data.1)[1], y = X, color=Y, shape=Y))+
scale_shape_manual(values = seq(from = 1, to = n)) +
guides(shape=guide_legend(nrow = 2)) +
xlab("Time (15-min)") + ylab("Počet áut") +
theme(panel.background=element_rect(fill = "white", colour = "black"),
legend.position="top",
legend.title=element_blank())
ggplotly(p)
res.wcc <- Func.motif(ts = pac$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc
pac
res.wcc <- Func.motif(ts = pac$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = pac$Index,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = pac$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.ahu <- Func.motif(ts = pac$SP12, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = dipu.pre_data$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = dipu.fil_data$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = dipu.pac_data$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = dipu.pac_data$SUM,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.wcc <- Func.motif(ts = dipu.pac_data$SUM)
res.wcc <- Func.motif(ts = dipu.pac_data$SUM,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
data(BuildOperation)
res.wcc <- Func.motif(ts = BuildOperation$WCC, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)
typeof(BuildOperation$WCC)
length(BuildOperation$WCC)
c(1:672)
xx<-c(1:672)
res.wcc <- Func.motif(ts = xx,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
library(readr)
library(ggplot2)
library(plotly)
library(stringr)
library(DataLoader)
library(forecast)
library(tseries)
library(zoo)
library(xts)
library(fts)
library(MASS)
library(caret)
library(e1071)
library(dplyr)
#library(h2o)        # Awesome ML Library
library(timetk)     # Toolkit for working with time series in R
library(tidyquant)
library(anomalyDetection)
library(TSMining)
library(randomForest)
#install.packages("devtools")
#devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
packageVersion('plotly')
# path to folder that holds multiple .csv files
#folder <- "C:/Users/USER/Documents/DP/Dataframe_Script/same_date/"
folder <- "./"
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder
# read in each .csv file in file_list and rbind them into a data frame called data1
data1 <-
do.call("rbind",
lapply(file_list,
function(x)
read_csv(paste(folder, x, sep=''))))
#====================Preprocesing===============================
#chnge format of Date
data1$Date_Time <- data1$Date
data1$Date_Time <- do.call(paste,c(data1[c("Date_Time","Time")],sep = ""))
data1$Date_Time <- as.POSIXct(data1$Date_Time,format = "%d%m%y%H%M")
data1$Date_Time <- as.Date(data1$Date_Time, "%d%m%y%H%M")
#data1$Date <- as.POSIXct(data1$Date,format = "%d%m%y")
data1<-data1[,c(ncol(data1),1:(ncol(data1)-1))]
#Add ":" in time
data1$Time <- sub("(.{2})(.*)", "\\1:\\2", data1$Time)
#Necessarily Columns as.factor
data1$CHANNEL <- as.numeric(data1$CHANNEL)
data1$LOCATION <- as.factor(data1$LOCATION)
data1$SITE <- as.factor(data1$SITE)
data1$FILENAME <- as.factor(data1$FILENAME)
data1$INSTRUMENT <- as.factor(data1$INSTRUMENT)
data1$HEADINGS <- as.factor(data1$HEADINGS)
#data1$Time <- as.numeric(data1$Time)
#set measured data as numeric
data1$SP1 = as.numeric(data1$SP1)
data1$SP2 = as.numeric(data1$SP2)
data1$SP3 = as.numeric(data1$SP3)
data1$SP4 = as.numeric(data1$SP4)
data1$SP5 = as.numeric(data1$SP5)
data1$SP6 = as.numeric(data1$SP6)
data1$SP7 = as.numeric(data1$SP7)
data1$SP8 = as.numeric(data1$SP8)
data1$SP9 = as.numeric(data1$SP9)
data1$SP10 = as.numeric(data1$SP10)
data1$SP11 = as.numeric(data1$SP11)
data1$SP12 = as.numeric(data1$SP12)
data1$SP13 = as.numeric(data1$SP13)
data1$SP14 = as.numeric(data1$SP14)
data1$LN1 = as.numeric(data1$LN1)
data1$LN2 = as.numeric(data1$LN2)
data1$LN3 = as.numeric(data1$LN3)
data1$LN4 = as.numeric(data1$LN4)
data1$LN5 = as.numeric(data1$LN5)
data1$LN6 = as.numeric(data1$LN6)
data1$LN7 = as.numeric(data1$LN7)
data1$LN8 = as.numeric(data1$LN8)
data1$LN9 = as.numeric(data1$LN9)
data1$CS1 = as.numeric(data1$CS1)
data1$CS2 = as.numeric(data1$CS2)
data1$CS3 = as.numeric(data1$CS3)
data1$CS4 = as.numeric(data1$CS4)
data1$CS5 = as.numeric(data1$CS5)
data1$CS6 = as.numeric(data1$CS6)
#add column sum of cars in interval
data1$SUM<-rowSums(data1[,5:18])
#unfinished preparation I will explain what I need
splitH<-str_split_fixed(data1$HEADINGS, " ", 4)
splitH <- as.data.frame(splitH)
splitH <- unique(splitH)
data1$CHANNEL1 <- data1$CHANNEL
colnames(splitH) <- "CHANNEL1"
colnames(splitH)[2] <- "CHANNEL2"
colnames(splitH)[3] <- "CHANNEL3"
colnames(splitH)[4] <- "CHANNEL4"
splitH$rowNames <-row.names.data.frame(splitH)
# replace.if
#
# for (i in vector) {
# data1$CHANNEL<-gsub("1", splitH[1,1], data1$CHANNEL)
# data1$CHANNEL<-gsub("2", splitH[1,2], data1$CHANNEL)
# data1$CHANNEL<-gsub("3", splitH[1,3], data1$CHANNEL)
# data1$CHANNEL<-gsub("4", splitH[1,4], data1$CHANNEL)
# }
#data1$CHANNEL <- do.call(paste,c(data1[c("CHANNEL1","CHANNEL")],sep = "_"))
data1$CHANNEL = as.factor(data1$CHANNEL)
data1$Time = as.factor(data1$Time)
data1$CHANNEL1 <- NULL
#===========================AGGREGATION==========================
filter<-filter(data1, CHANNEL == "PP",FILENAME == "SC1_MEJA")
df_XCV<-data1
abc <- df_XCV$Date_Time
df_XCV<-select_if(df_XCV, is.numeric)
df_XCV$Date_Time<- abc
df_XCV<-df_XCV[,c(ncol(df_XCV),1:(ncol(df_XCV)-1))]
#Create time series object
df_XCV_xts<-xts(df_XCV[, -1], order.by=as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv("TZ")))
ep <- endpoints(df_XCV_xts, on = "hours")
pac<-period.apply(df_XCV_xts[,(names(df_XCV_xts)) ], INDEX = ep, FUN = mean)
#Time series to DF
pac <-fortify(pac)
pac <-  select(pac, -c(PEAKINT, INTERVAL))
pac<-tk_augment_timeseries_signature(pac)
#==================================Motif discovery=======================================
res.wcc <- Func.motif(ts = pac$SP11,global.norm = T,local.norm = F,window.size = 24,overlap = 0,w = 6,a = 5,mask.size = 5,max.dist.ratio = 1.2,count.ratio.1 = 1.1,count.ratio.2 = 1.1)
res.ahu <- Func.motif(ts = pac$SP12, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)
#Visualization
data.wcc <- Func.visual.SingleMotif(single.ts = pac$SP11, window.size = 24, motif.indices = res.wcc$Indices)
data.ahu <- Func.visual.SingleMotif(single.ts = pac$SP12, window.size = 24, motif.indices = res.ahu$Indices)
n <- length(unique(data.wcc$data.1$Y))
#Make the plot
p<-ggplot(data = data.wcc$data.1) +
geom_line(aes(x = 1:dim(data.wcc$data.1)[1], y = X)) +
geom_point(aes(x = 1:dim(data.wcc$data.1)[1], y = X, color=Y, shape=Y))+
scale_shape_manual(values = seq(from = 1, to = n)) +
guides(shape=guide_legend(nrow = 2)) +
xlab("Time (15-min)") + ylab("Počet áut") +
theme(panel.background=element_rect(fill = "white", colour = "black"),
legend.position="top",
legend.title=element_blank())
ggplotly(p)
#==================================PREDICTION=======================================
shiny::runApp()
runApp()
runApp()
