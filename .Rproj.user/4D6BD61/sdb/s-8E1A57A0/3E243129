{
    "collab_server" : "",
    "contents" : "library(readr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(stringr)\nlibrary(DataLoader)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(zoo)\nlibrary(xts)\nlibrary(fts)\nlibrary(MASS)\nlibrary(caret)\nlibrary(e1071)\nlibrary(dplyr)\n#library(h2o)        # Awesome ML Library\nlibrary(timetk)     # Toolkit for working with time series in R\nlibrary(tidyquant)\nlibrary(anomalyDetection)\nlibrary(TSMining)\n#install.packages(\"devtools\")\ndevtools::install_github(\"twitter/AnomalyDetection\")\nlibrary(anomalyDetection)\npackageVersion('plotly')\n\n\n# path to folder that holds multiple .csv files\n#folder <- \"C:/Users/USER/Documents/DP/Dataframe_Script/same_date/\"      \nfolder <- \"./\"\nfile_list <- list.files(path=folder, pattern=\"*.csv\") # create list of all .csv files in folder\n# read in each .csv file in file_list and rbind them into a data frame called data1 \ndata1 <- \n  do.call(\"rbind\", \n          lapply(file_list, \n                 function(x) \n                   read_csv(paste(folder, x, sep=''))))\n\n\n#====================Preprocesing===============================\n#chnge format of Date\ndata1$Date_Time <- data1$Date\ndata1$Date_Time <- do.call(paste,c(data1[c(\"Date_Time\",\"Time\")],sep = \"\"))\ndata1$Date_Time <- as.POSIXct(data1$Date_Time,format = \"%d%m%y%H%M\")\n#data1$Date_Time <- as.Date(data1$Date_Time, \"%d%m%y%H%M\")\n#data1$Date <- as.POSIXct(data1$Date,format = \"%d%m%y\")\ndata1<-data1[,c(ncol(data1),1:(ncol(data1)-1))]\n\n\n\n#Add \":\" in time\ndata1$Time <- sub(\"(.{2})(.*)\", \"\\\\1:\\\\2\", data1$Time)\n\n\n#Necessarily Columns as.factor\ndata1$CHANNEL <- as.numeric(data1$CHANNEL)\ndata1$LOCATION <- as.factor(data1$LOCATION)\ndata1$SITE <- as.factor(data1$SITE)\ndata1$FILENAME <- as.factor(data1$FILENAME)\ndata1$INSTRUMENT <- as.factor(data1$INSTRUMENT)\ndata1$HEADINGS <- as.factor(data1$HEADINGS)\n#data1$Time <- as.numeric(data1$Time)\n#set measured data as numeric\ndata1$SP1 = as.numeric(data1$SP1)\ndata1$SP2 = as.numeric(data1$SP2) \ndata1$SP3 = as.numeric(data1$SP3) \ndata1$SP4 = as.numeric(data1$SP4) \ndata1$SP5 = as.numeric(data1$SP5) \ndata1$SP6 = as.numeric(data1$SP6) \ndata1$SP7 = as.numeric(data1$SP7)\ndata1$SP8 = as.numeric(data1$SP8) \ndata1$SP9 = as.numeric(data1$SP9) \ndata1$SP10 = as.numeric(data1$SP10) \ndata1$SP11 = as.numeric(data1$SP11)\ndata1$SP12 = as.numeric(data1$SP12) \ndata1$SP13 = as.numeric(data1$SP13) \ndata1$SP14 = as.numeric(data1$SP14) \n\ndata1$LN1 = as.numeric(data1$LN1)\ndata1$LN2 = as.numeric(data1$LN2)\ndata1$LN3 = as.numeric(data1$LN3)\ndata1$LN4 = as.numeric(data1$LN4)\ndata1$LN5 = as.numeric(data1$LN5)\ndata1$LN6 = as.numeric(data1$LN6)\ndata1$LN7 = as.numeric(data1$LN7)\ndata1$LN8 = as.numeric(data1$LN8)\ndata1$LN9 = as.numeric(data1$LN9)\n\ndata1$CS1 = as.numeric(data1$CS1)\ndata1$CS2 = as.numeric(data1$CS2)\ndata1$CS3 = as.numeric(data1$CS3)\ndata1$CS4 = as.numeric(data1$CS4)\ndata1$CS5 = as.numeric(data1$CS5)\ndata1$CS6 = as.numeric(data1$CS6)\n  \n  \n#add column sum of cars in interval\ndata1$SUM<-rowSums(data1[,5:18])\n\n#unfinished preparation I will explain what I need\nsplitH<-str_split_fixed(data1$HEADINGS, \" \", 4)\nsplitH <- as.data.frame(splitH)\nsplitH <- unique(splitH)\ndata1$CHANNEL1 <- data1$CHANNEL\ncolnames(splitH) <- \"CHANNEL1\"\ncolnames(splitH)[2] <- \"CHANNEL2\"\ncolnames(splitH)[3] <- \"CHANNEL3\"\ncolnames(splitH)[4] <- \"CHANNEL4\"\nsplitH$rowNames <-row.names.data.frame(splitH)\n# replace.if\n# \n# for (i in vector) {\n# data1$CHANNEL<-gsub(\"1\", splitH[1,1], data1$CHANNEL)\n# data1$CHANNEL<-gsub(\"2\", splitH[1,2], data1$CHANNEL)\n# data1$CHANNEL<-gsub(\"3\", splitH[1,3], data1$CHANNEL)\n# data1$CHANNEL<-gsub(\"4\", splitH[1,4], data1$CHANNEL)\n# }\n\n\ndata1$CHANNEL <- do.call(paste,c(data1[c(\"CHANNEL1\",\"CHANNEL\")],sep = \"_\"))\ndata1$CHANNEL = as.factor(data1$CHANNEL)\ndata1$Time = as.factor(data1$Time)\ndata1$CHANNEL1 <- NULL\n\n\n#===========================AGGREGATION==========================\n\nfilter<-filter(data1, CHANNEL == \"PP\",FILENAME == \"SC1_MEJA\")\n\ndf_XCV<-data1\nabc <- df_XCV$Date_Time\ndf_XCV<-select_if(df_XCV, is.numeric)\ndf_XCV$Date_Time<- abc\ndf_XCV<-df_XCV[,c(ncol(df_XCV),1:(ncol(df_XCV)-1))]\n#Create time series object\ndf_XCV_xts<-xts(df_XCV[, -1], order.by=as.POSIXct(df_XCV$Date_Time,tzone = Sys.getenv(\"TZ\")))\n\nep <- endpoints(df_XCV_xts, on = \"hours\")\npac<-period.apply(df_XCV_xts[,(names(df_XCV_xts)) ], INDEX = ep, FUN = mean)\n#Time series to DF\npac <-fortify(pac)\npac <-  select(pac, -c(PEAKINT, INTERVAL))\npac<-tk_augment_timeseries_signature(pac)\n#source(\"Aggregation_Date.R\")\n#source(\"Aggregation_Date_Time.R\")\n#===========================PLOTS==========================\n\nx <- ggplot(data1,aes(x = Date, y = SUM,col = CHANNEL))+\n  geom_bar(stat = \"identity\",size = 3,alpha = 0.5,fill = \"blue\")\n\nx<-ggplot(data1,aes(x = Date, y = SUM,col = CHANNEL))+\n  geom_line(size = 0.5,alpha = 0.6)\n\np<-SUM_DATA %>%\n  ggplot(aes(x = Index, y = SUM)) \nplot_ly(SUM_DATA, x = ~Index, y = ~SUM)%>%\n  add_lines(alpha = 0.6)\n\nggplotly(x)  \n\nggplot(SUM_DATA,aes(x=Index,y=SUM))+\n  # Training data\n  geom_line(color = palette_light()[[1]]) +\n  geom_point(color = palette_light()[[1]])+\n  # Predictions\n  geom_line(aes(y = value), color = palette_light()[[2]], data = predictions_tbl) +\n  geom_point(aes(y = value), color = palette_light()[[2]], data = predictions_tbl)+ \n  # Actuals\n  geom_line(color = palette_light()[[1]], data = actuals_tbl) +\n  geom_point(color = palette_light()[[1]], data = actuals_tbl)+\n  theme_tq() +\n  labs(title = \"Time series sum data\")\n#Plot of aggregation by SpeedBins\nplot_ly(pac, x = ~Index, y = ~SP1 , name = 'Number of cars in time aggregated')%>%\n  add_trace(y = ~SP1, name = '0-50km/h', mode = 'lines') %>%\n  add_trace(y = ~SP2, name = '50-60km/h', mode = 'lines') %>%\n  add_trace(y = ~SP3, name = '60-70km/h', mode = 'lines') %>%\n  add_trace(y = ~SP4, name = '70-80km/h', mode = 'lines') %>%\n  add_trace(y = ~SP5, name = '80-90km/h', mode = 'lines') %>%\n  add_trace(y = ~SP6, name = '90-100km/h', mode = 'lines') %>%\n  add_trace(y = ~SP7, name = '100-110km/h', mode = 'lines') %>%\n  add_trace(y = ~SP8, name = '110-120km/h', mode = 'lines') %>%\n  add_trace(y = ~SP9, name = '120-130km/h', mode = 'lines') %>%\n  add_trace(y = ~SP10, name = '130-140km/h', mode = 'lines') %>%\n  add_trace(y = ~SP11, name = '140-150km/h', mode = 'lines') %>%\n  add_trace(y = ~SP12, name = '150-160km/h', mode = 'lines') %>%\n  add_trace(y = ~SP13, name = '160-180km/h', mode = 'lines')%>%\n  add_trace(y = ~SP14, name = '180-999km/h', mode = 'lines')%>%\n  add_trace(y = ~SUM, name = 'TOTAL', mode = 'lines')\n\n#Plot of aggregation by LengthBins\nplot_ly(pac, x = ~Index, y = ~LN1 , name = 'Number of cars in time aggregated')%>%\n  add_trace(y = ~LN1, name = '0-300 cm', mode = 'lines') %>%\n  #add_trace(y = ~LN2, name = '300-470 cm', mode = 'lines') %>%\n  add_trace(y = ~LN3, name = '470-550 cm', mode = 'lines') %>%\n  add_trace(y = ~LN4, name = '550-600 cm', mode = 'lines') %>%\n  add_trace(y = ~LN5, name = '600-1300 cm', mode = 'lines') %>%\n  add_trace(y = ~LN6, name = '1300-1800 cm', mode = 'lines') %>%\n  add_trace(y = ~LN7, name = '1800-2550 cm', mode = 'lines') %>%\n  add_trace(y = ~LN8, name = '2550-3600 cm', mode = 'lines') %>%\n  add_trace(y = ~LN9, name = '3600-9999 cm', mode = 'lines') \n\n\n#Plot of aggregation by WeigthBins\nplot_ly(agg_CS, x = ~Date, y = ~CS1 , name = 'Number of cars in time aggregated')%>%\n  add_trace(y = ~CS1, name = '0-2000 kg', mode = 'lines') %>%\n  #add_trace(y = ~CS2, name = '2000-3501 kg', mode = 'lines') %>%\n  add_trace(y = ~CS3, name = '3501-7497 kg', mode = 'lines') %>%\n  add_trace(y = ~CS4, name = '7497-12002 kg', mode = 'lines') %>%\n  add_trace(y = ~CS5, name = '12002-17998  kg', mode = 'lines') %>%\n  add_trace(y = ~CS6, name = '17998-23999  kg', mode = 'lines') \n  \n\n#Bar plot day of week\nplot_ly(pac, x = ~wday.lbl, y = ~SP1, type = 'bar', name = \"0-50km/h\") %>%\n  add_trace(y = ~SP2, name = '50-60km/h', mode = 'lines') %>%\n  add_trace(y = ~SP3, name = '60-70km/h', mode = 'lines') %>%\n  add_trace(y = ~SP4, name = '70-80km/h', mode = 'lines') %>%\n  add_trace(y = ~SP5, name = '80-90km/h', mode = 'lines') %>%\n  add_trace(y = ~SP6, name = '90-100km/h', mode = 'lines') %>%\n  add_trace(y = ~SP7, name = '100-110km/h', mode = 'lines') %>%\n  add_trace(y = ~SP8, name = '110-120km/h', mode = 'lines') %>%\n  add_trace(y = ~SP9, name = '120-130km/h', mode = 'lines') %>%\n  add_trace(y = ~SP10, name = '130-140km/h', mode = 'lines') %>%\n  add_trace(y = ~SP11, name = '140-150km/h', mode = 'lines') %>%\n  add_trace(y = ~SP12, name = '150-160km/h', mode = 'lines') %>%\n  add_trace(y = ~SP13, name = '160-180km/h', mode = 'lines')%>%\n  add_trace(y = ~SP14, name = '180-999km/h', mode = 'lines')%>%\n  #add_trace(y = ~SUM, name = 'TOTAL', mode = 'lines')%>%\n  layout(title = \"Počty áuto podľa dni v týždni\",yaxis = list(title = 'Count'), barmode = 'group')\n\n\n#pie plot by day of week\n  plot_ly(pac, labels = ~wday.lbl, values = ~SP10, type = 'pie',textposition = 'inside') %>%\n  layout(title = 'Kol??ov? graf')\n  \n#boxplot\n plot_ly(pac,y = ~SP1, name = '0-50km/h',type = 'box')%>%\n   add_trace(y = ~SP2, name = '50-60km/h', mode = 'lines') %>%\n   add_trace(y = ~SP3, name = '60-70km/h', mode = 'lines') %>%\n   add_trace(y = ~SP4, name = '70-80km/h', mode = 'lines') %>%\n   add_trace(y = ~SP5, name = '80-90km/h', mode = 'lines') %>%\n   add_trace(y = ~SP6, name = '90-100km/h', mode = 'lines') %>%\n   add_trace(y = ~SP7, name = '100-110km/h', mode = 'lines') %>%\n   add_trace(y = ~SP8, name = '110-120km/h', mode = 'lines') %>%\n   add_trace(y = ~SP9, name = '120-130km/h', mode = 'lines') %>%\n   add_trace(y = ~SP10, name = '130-140km/h', mode = 'lines') %>%\n   add_trace(y = ~SP11, name = '140-150km/h', mode = 'lines') %>%\n   add_trace(y = ~SP12, name = '150-160km/h', mode = 'lines') %>%\n   add_trace(y = ~SP13, name = '160-180km/h', mode = 'lines')%>%\n   add_trace(y = ~SP14, name = '180-999km/h', mode = 'lines')%>%\n    layout(title = \"Boxplot by day of week\")\n \n plot_ly(data1,x = ~CHANNEL,y = ~SP11, color = ~wday.lbl,type = \"box\")%>%\n   layout(boxmode = \"group\")\n\n #==================================Anomaly detection=======================================\n SUM_DATA <- pac[,c(1,12)]\n abc <- as.numeric(rownames(SUM_DATA))\n ggplot(pac, aes(x=Index, y=SP11)) + geom_line()\n \n res = AnomalyDetectionTs(SUM_DATA, max_anoms=0.01, direction=\"pos\", plot=TRUE, e_value = T)\n \n res = AnomalyDetectionVec(SUM_DATA[,2], max_anoms=0.01, period=96, direction='both',\n                           only_last=FALSE, plot=TRUE)\n\n anomaly_table=res$anoms\n SUM_DATA$Index_row <- abc\n anomaly_table<-merge(SUM_DATA,anomaly_table,by.x = \"Index_row\",by.y = \"index\")\n \n \n \n \n plot_ly(SUM_DATA, x = ~Index, y = ~SP11)%>%\n   add_trace(colors = \"orange\",name = \"Po?etnos? ?ut v ?ase\",mode = \"lines\")%>%\n   add_trace(y = ~anoms, colors = \"gray\",name = \"Anom?lie\", mode = \"markers\", alpha = 1,data = anomaly_table)%>%\n   layout(title = \"Graf\",\n          xaxis = list(title = \"Čas\",\n                       rangeslider = list(type = \"date\")),\n          yaxis = list(title = \"Početnosť áut\"))\n #==================================Motif discovery=======================================\n \n res.wcc <- Func.motif(ts = pac$SP11, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)\n \n res.ahu <- Func.motif(ts = pac$SP12, global.norm = T, local.norm = F, window.size = 24, overlap = 0, w = 6, a = 5, mask.size = 5, max.dist.ratio = 1.2, count.ratio.1 = 1.1, count.ratio.2 = 1.1)\n \n \n #Visualization\n data.wcc <- Func.visual.SingleMotif(single.ts = pac$SP11, window.size = 24, motif.indices = res.wcc$Indices)\n data.ahu <- Func.visual.SingleMotif(single.ts = pac$SP12, window.size = 24, motif.indices = res.ahu$Indices)\n \n #Determine the total number of motifs discovered in the time series of WCC\n n <- length(unique(data.wcc$data.1$Y))\n #Make the plot\n p<-ggplot(data = data.wcc$data.1) +  \n   geom_line(aes(x = 1:dim(data.wcc$data.1)[1], y = X)) +\n   geom_point(aes(x = 1:dim(data.wcc$data.1)[1], y = X, color=Y, shape=Y))+\n   scale_shape_manual(values = seq(from = 1, to = n)) +\n   guides(shape=guide_legend(nrow = 2)) +\n   xlab(\"Time (15-min)\") + ylab(\"Počet áut\") +\n   theme(panel.background=element_rect(fill = \"white\", colour = \"black\"),\n         legend.position=\"top\",\n         legend.title=element_blank())\n \n ggplotly(p)\n#==================================PREDICTION=======================================\ncount_ts = ts(pac[, c('SUM')])\npac$clean_cnt = tsclean(count_ts)\n\npac$cnt_ma = ma(pac$SUM, order=7) # using the clean count with no outliers\npac$cnt_ma30 = ma(pac$SUM, order=30)\n\n\n l<-ggplot() +\n  geom_line(data = pac, aes(x = Index, y = clean_cnt, colour = \"Counts\")) +\n  geom_line(data = pac, aes(x = Index, y = cnt_ma,   colour = \"Weekly Moving Average\"))  +\n  geom_line(data = pac, aes(x = Index, y = cnt_ma30, colour = \"Monthly Moving Average\"))  +\n  ylab('Cars Count')\n \n ggplotly(l)\n\n \n \n \n \n count_ma = ts(na.omit(pac$SUM), frequency=3)\n decomp = stl(count_ma, s.window=\"periodic\")\n deseasonal_cnt <- seasadj(decomp)\n k<-plot(decomp)\n\n adf.test(count_ma, alternative = \"stationary\")\n\n Acf(count_ma, main='') \n\n Pacf(count_ma, main='')  \n \n \n count_d1 = diff(deseasonal_cnt, differences = 1)\n plot(count_d1)\n adf.test(count_d1, alternative = \"stationary\")\n\n \n Acf(count_d1, main='ACF for Differenced Series')\n Pacf(count_d1, main='PACF for Differenced Series') \n \n \n auto.arima(deseasonal_cnt, seasonal=FALSE)\n\n fit<-auto.arima(deseasonal_cnt, seasonal=FALSE)\n tsdisplay(residuals(fit), lag.max=45, main='(1,1,1) Model Residuals') \n\n fit2 = arima(deseasonal_cnt, order=c(1,1,7))\n \n fit2\n \n tsdisplay(residuals(fit2), lag.max=15, main='Seasonal Model Residuals') \n\n fcast <- forecast(fit2, h=30)\n plot(fcast) \n\n \n hold <- window(ts(deseasonal_cnt), start=700)\n \n fit_no_holdout = arima(ts(deseasonal_cnt[-c(700:725)]), order=c(1,1,7))\n \n fcast_no_holdout <- forecast(fit_no_holdout,h=25)\n plot(fcast_no_holdout, main=\" \")\n lines(ts(deseasonal_cnt)) \n\n \n fit_w_seasonality = auto.arima(deseasonal_cnt, seasonal=TRUE)\n fit_w_seasonality \n seas_fcast <- forecast(fit_w_seasonality, h=30)\n plot(seas_fcast) \n #=========================ARMA========================\n adf.test(diff(log(pac$SUM)), alternative=\"stationary\", k=0)\n acf(log(pac$SUM))\n acf(diff(log(pac$SUM)))\n pacf(diff(log(pac$SUM)))\n (fit <- arima(log(pac$SUM), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))\n pred <- predict(fit, n.ahead = 10*12)\n ts.plot(as.ts(pac$Index),2.718^pred$pred, log = \"y\", lty = c(1,3))\n #=========================MACHINE LEARNING========================\n \n # Shuffle row indices: rows\n rows <- sample(nrow(data1))\n \n # Randomly order data: Sonar\n data1 <- data1[rows,]\n \n # Identify row to split on: split\n split <- round(nrow(data1) * .60)\n \n # Create train\n train <- data1[1:split,]\n \n # Create test\n test <- data1[(split + 1):nrow(data1),]\n#------------------------------------------\n \n # Run algorithms using 10-fold cross validation\n control <- trainControl(method=\"cv\", number=10)\n metric <- \"Accuracy\"\n validation_index <- createDataPartition(data1$CHANNEL, p=0.80, list=FALSE)\n # select 20% of the data for validation\n validation <- data1[-validation_index,]\n # use the remaining 80% of data to training and testing the models\n dataset <- data1[validation_index,]\n # a) linear algorithms\n set.seed(7)\n fit.lda <- train(SP10 ~ Date_Time,data=data1, method=\"lda\", metric=metric, trControl=control)\n # b) nonlinear algorithms\n # CART\n set.seed(7)\n fit.cart <- train(SP10 ~ Date_Time,data=data1, method=\"rpart\", metric=metric, trControl=control)\n # kNN\n set.seed(7)\n fit.knn <- train(SP10 ~ Date_Time,data=data1, method=\"knn\", metric=metric, trControl=control)\n # c) advanced algorithms\n # SVM\n set.seed(7)\n fit.svm <- train(SP10 ~ Date_Time,data=data1, method=\"svmRadial\", metric=metric, trControl=control)\n # Random Forest\n set.seed(7)\n fit.rf <- train(SP10 ~ Date_Time,data=data1, method=\"rf\", metric=metric, trControl=control)\n \n \n #summarize accuracy of models\n results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))\n summary(results)\n dotplot(results)\n print(fit.lda)\n \n \n # estimate skill of LDA on the validation dataset\n predictions <- predict(fit.lda, validation)\n confusionMatrix(predictions, validation$CHANNEL)\n ",
    "created" : 1522299734434.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3834011429",
    "id" : "3E243129",
    "lastKnownWriteTime" : 1522351438,
    "last_content_update" : 1522351438702,
    "path" : "~/fiverr/Rshiny/abc.r",
    "project_path" : "abc.r",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}